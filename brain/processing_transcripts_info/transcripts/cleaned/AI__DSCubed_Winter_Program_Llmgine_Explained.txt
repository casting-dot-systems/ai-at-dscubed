Speaker: Lecturer

All right, welcome to this lecture on the LLM engine. Yesterday's recording broke, so here we are again. The goal of this lecture is to explain the LLM engine framework and set the specifications for a quick exercise project that you can do to get hands-on experience in understanding and building with the framework.

Last week, we covered a lot of introductory concepts, specifically software engineering concepts within Python—setting up the project structure, using UV as the package manager, Python typing, classes, and so on. There was also a big section on concurrency, which is very important for building fast applications. Now, we have modules related to LLMs themselves.

After completing your tool-calling project, you probably realized that getting an LLM response isn't that complicated. There's the conversation—a list of dictionaries you put in as messages—and then there are tool schemas you can define to include in the call. The LLM knows it can call those functions, and if it wants, it will send back an intent on which functions to call. Then, it's up to our application to call those functions and send back the results.

At the end of the day, all the LLM has to do is get context and get tools—two very simple things. Conceptually, it's simple, though implementing it requires getting the right tools and details. That's what your project one was about.

Our goal is to automate the entire club. To do that, we break down processes we can start automating. To automate things, we need an LLM that can take action in the world, which is done through function calls within these LLM calls that you have practiced. We've gained the capability to call LLMs and get them to respond with function requests. Now, we need to integrate that into a proper application.

As the complexity of the application grows, the architecture and the way we organize the code and logic become more important. If we just have a bunch of random classes, it will soon get overwhelming. We won't know where to put new features and files. That's the motivation for building LLM engine—a general framework and collection of patterns that helps us build LLM applications in an organized and efficient manner.

Before we dive in, realize that problems like calling functions, getting models, and getting context are problems others have tried to solve as well. As a result, they've built their own frameworks. For example, Llama Index makes it easy to load data and query it in just a couple of lines. These frameworks abstract out tedious details like defining conversations, models, calling APIs, and parsing responses, exposing you only to higher-level things that make it easy to build applications. That's what we aim to do as well.

However, the main problem with committing to one of these solutions—and the reason we chose to build our own—is that this field is very young, less than two years old. Best practices for building LLM applications are still being figured out. If we committed to any one framework, we'd be limited by what it does. If it doesn't have a feature we want, it's hard to contribute and get it implemented in a timely manner. That's why we chose to build our own framework, LLM engine. It's also great practice for us to learn about this new field through experimentation.

Again, it's not that complicated: we're just putting information in the prompt, giving it tools, and calling tools. These are not impossibly complicated things.

Now, let's go into the actual framework itself. [pause] I'm not sure if you can see this diagram well, but this is what we'll be explaining.

There are parts here you're already familiar with: context (chat history), the model you're calling from, and a mechanism to call tools (defining tool schemas and executing tools when the LLM wants to). The core object or class within LLM engine is a container we've called "engine." An engine is simply a class with an input and an output. This allows us to group relevant logic into one class.

Often, for a chatbot, the input would be human text and the output would be the LLM response. The engine can remember the conversation and handle tool calling, etc. It's really just a container for LLM logic.

The rest of the framework is this message bus structure. The reason for this is to decouple and reduce dependencies between different parts of our application. It's abstract, but I'll get into details in a second. There are really just two parts: the LLM logic and the application architecture to help us do certain things.

To demonstrate what we've built, in our first semester we built LLM engine and, as a proof of concept, a Notion CRUD Discord bot. CRUD stands for create, read, update, delete. It's a Discord bot called Darcy that can interact with tasks in Notion. For example, you can say, "Hey Darcy, what are my tasks?" and it will fetch your tasks live from Notion and generate a response.

Let me demo it. [pause] I've started the bot. It's able to fetch tasks, create tasks (with confirmation), and update tasks. For example, if I say, "Hey Darcy, can you create a test task for me?" it will ask for confirmation, and once confirmed, it will add the test task. If I say, "Darcy, I just finished that task," it will get the active task, ask for confirmation, and then mark it as done.

How did we do that? It's actually very simple. We're just giving the LLM context—specifically, context about the tools, which are functions we've defined to interact with Notion. For example, we have functions like get all users, get active projects, update task, etc. These are just functions hooked up to the LLM. The complicated part is actually the UI and integrating different components together.

That's what we did in the first semester: we built the framework and validated it through the application of building Darcy.

Now, I'll explain the structure. Let's go back to the diagram. [pause] We're going to go over each part of the engine.

Again, our core object within the framework is an entity called an engine—an encapsulation of logic. If we look at the Discord bot demo, it's within another repo. In the Notion CRUD engine v3, we define three objects at the top: the tool manager (which registers tools, creates schemas, and executes tools), the context manager (to store chat history), and the model (currently using 4.1 mini). Within the init of this engine, we're creating these three main parts of our logic. There are some helper commands as well.

Inside, the main logic intakes a command and returns a result (input and output). Within, you store the input, get the context, get the tools, and create a response. If there's a tool call, it executes the function. There's some additional logic for matching Notion IDs to names, but otherwise, it's similar. At the bottom, it's a loop: it keeps going until all the tools are used. For example, to assign a task to a project, it first fetches all project IDs, then calls the create task function. The engine keeps going until all functions are called, then produces an output.

That's an example of an engine.

Now, about the two repos: the point of LLM engine is that it's a general framework and collection of patterns. In theory, we should be able to apply it to other applications, not just for DS Cubed. So, we have one repo for everything general about LLM engine, and another repo just for DS Cubed-related stuff. I'll link those two for you to see. We sync them up by having them in the same parent directory. I'll explain that in more detail later.

Within the LLM engine repo, we have the code for the three things you saw: the tool manager (registering, getting, and executing tools), providers (for getting from OpenAI), and models (for getting the model for 4.1). We also have the chat history manager, which is similar to what you built—just appending stuff to an object.

What separates this from just doing that, and the main innovation of the framework, is the message bus data structure. The bus allows us to decouple parts of the application from each other.

Before I demo, let me explain what this is. The message bus is a class (in bus.py) that can route messages from one object to another or to a function. Messages are simply data classes. There are events and commands. An event is just a data class with an event ID, timestamp, metadata, and session ID. You define your custom event by inheriting from this class. For example, tool chat status event inherits from the event class, and you can define whatever data structure you want on the custom event.

Commands are similar—just data classes we're passing around. This ties back to why we learned about data classes last week.

The bus takes in events and commands. It does this through a function called publish for events (message_bus.publish) and execute for commands (message_bus.execute). These functions take in the event or command and route it to certain parts of the application that we predefine.

One application of this is the UI. For example, with the Notion CRUD engine, we can talk to it through Discord or through our CLI interface. The code to output in the CLI and Discord is the same—it's just this class. How is it able to do that? Traditionally, you'd imagine there would be a line like discord.print or similar, but because we have this message bus, we're able to send out events and commands to different parts of the application without changing the internal logic of the engine.

For example, while it's loading, the engine sends out a loading status event. The bus routes that event to the relevant component within the display (CLI or Discord), which then shows the display. When running in the CLI, the UI that's active is the CLI interface; when running in Discord, it's the Discord chat interface. The